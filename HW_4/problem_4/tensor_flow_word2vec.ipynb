{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "import collections\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import random\n",
    "from tempfile import gettempdir\n",
    "import zipfile\n",
    "\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builti\n",
    "\n",
    "from tensorflow.contrib.tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.path.dirname(os.path.realpath(sys.argv[0]))\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--log_dir',\n",
    "    type=str,\n",
    "    default=os.path.join(current_path, 'log'),\n",
    "    help='The log directory for TensorBoard summaries.')\n",
    "FLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "# Create the directory for TensorBoard variables if there is not.\n",
    "if not os.path.exists(FLAGS.log_dir):\n",
    "  os.makedirs(FLAGS.log_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "twentyng_data = fetch_20newsgroups(subset='train',remove=('headers','footers','quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tNG_data = twentyng_data.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(s):\n",
    "    tr = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    ##s = \"Hello! It is time to remove punctuations. It is easy, you will see.\"\n",
    "    s = s.translate(tr)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tNG_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "low = []\n",
    "for line in tNG_data:\n",
    "    line = line.strip() # skip the empty line\n",
    "    line = line.replace('\\n',' ') # replaces new lines to empty spaces\n",
    "    #line = line.lstrip()\n",
    "    line = remove_punctuation(line)\n",
    "    lines=line.split()\n",
    "    #print(lines)\n",
    "    low.extend(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 50000\n",
    "\n",
    "#\"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "def build_dataset(words, n_words):\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        index = dictionary.get(word, 0)\n",
    "        if index == 0:  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 74139], ('the', 94602), ('to', 51811), ('of', 45906), ('a', 40446)]\n",
      "Sample data [8, 23, 1225, 33, 144, 50, 42, 89, 10777, 46] ['I', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me']\n"
     ]
    }
   ],
   "source": [
    "# Filling 4 global variables:\n",
    "# data - list of codes (integers from 0 to vocabulary_size-1).\n",
    "#   This is the original text but words are replaced by their codes\n",
    "# count - map of words(strings) to count of occurrences\n",
    "# dictionary - map of words(strings) to their codes(integers)\n",
    "# reverse_dictionary - maps codes(integers) to words(strings)\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(\n",
    "    vocabulary, vocabulary_size)\n",
    "del vocabulary  # Hint to reduce memory.\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])\n",
    "\n",
    "data_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Function to generate a training batch for the skip-gram model.\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "  global data_index\n",
    "  assert batch_size % num_skips == 0\n",
    "  assert num_skips <= 2 * skip_window\n",
    "  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "  span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "  buffer = collections.deque(maxlen=span)\n",
    "  if data_index + span > len(data):\n",
    "    data_index = 0\n",
    "  buffer.extend(data[data_index:data_index + span])\n",
    "  data_index += span\n",
    "  for i in range(batch_size // num_skips):\n",
    "    context_words = [w for w in range(span) if w != skip_window]\n",
    "    words_to_use = random.sample(context_words, num_skips)\n",
    "    for j, context_word in enumerate(words_to_use):\n",
    "      batch[i * num_skips + j] = buffer[skip_window]\n",
    "      labels[i * num_skips + j, 0] = buffer[context_word]\n",
    "    if data_index == len(data):\n",
    "      buffer.extend(data[0:span])\n",
    "      data_index = span\n",
    "    else:\n",
    "      buffer.append(data[data_index])\n",
    "      data_index += 1\n",
    "  # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "  data_index = (data_index + len(data) - span) % len(data)\n",
    "  return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1225]\n",
      " [   8]\n",
      " [  23]\n",
      " [  33]\n",
      " [1225]\n",
      " [ 144]\n",
      " [  50]\n",
      " [  33]]\n",
      "23 was -> 1225 wondering\n",
      "23 was -> 8 I\n",
      "1225 wondering -> 23 was\n",
      "1225 wondering -> 33 if\n",
      "33 if -> 1225 wondering\n",
      "33 if -> 144 anyone\n",
      "144 anyone -> 50 out\n",
      "144 anyone -> 33 if\n"
     ]
    }
   ],
   "source": [
    "batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)\n",
    "print(labels)\n",
    "for i in range(8):\n",
    "    print(batch[i], reverse_dictionary[batch[i]], '->', labels[i, 0],reverse_dictionary[labels[i, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-15-53cf61cc96d5>:68: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Build and train a skip-gram model.\n",
    "\n",
    "batch_size = 128\n",
    "embedding_size = 300  # Dimension of the embedding vector.\n",
    "skip_window = 1  # How many words to consider left and right.\n",
    "num_skips = 2  # How many times to reuse an input to generate a label.\n",
    "num_sampled = 64  # Number of negative examples to sample.\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent. These 3 variables are used only for\n",
    "# displaying model accuracy, they don't affect calculation.\n",
    "valid_size = 16  # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "#valid_examples = np.random.choice(50000, 50000, replace=False)\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  with tf.name_scope('inputs'):\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "  # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "  with tf.device('/cpu:0'):\n",
    "    # Look up embeddings for inputs.\n",
    "    with tf.name_scope('embeddings'):\n",
    "        embeddings = tf.Variable(\n",
    "          tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "    # Construct the variables for the NCE loss\n",
    "    with tf.name_scope('weights'):\n",
    "        nce_weights = tf.Variable(\n",
    "          tf.truncated_normal(\n",
    "              [vocabulary_size, embedding_size],\n",
    "              stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    with tf.name_scope('biases'):\n",
    "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  # Compute the average NCE loss for the batch.\n",
    "  # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "  # time we evaluate the loss.\n",
    "  # Explanation of the meaning of NCE loss:\n",
    "  #   http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "  with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.nce_loss(\n",
    "            weights=nce_weights,\n",
    "            biases=nce_biases,\n",
    "            labels=train_labels,\n",
    "            inputs=embed,\n",
    "            num_sampled=num_sampled,\n",
    "            num_classes=vocabulary_size))\n",
    "\n",
    "  # Add the loss value as a scalar to summary.\n",
    "  tf.summary.scalar('loss', loss)\n",
    "\n",
    "  # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "  with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "  # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "  normalized_embeddings = embeddings / norm\n",
    "  valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings,\n",
    "                                            valid_dataset)\n",
    "  similarity = tf.matmul(\n",
    "      valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "  # Merge all summaries.\n",
    "  merged = tf.summary.merge_all()\n",
    "\n",
    "  # Add variable initializer.\n",
    "  init = tf.global_variables_initializer()\n",
    "\n",
    "  # Create a saver.\n",
    "  saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step  0 :  291.2022705078125\n",
      "Nearest to The: 17, CONSTRUED, suggestionscomments, underwear, microwave, supreme, decay, forgot,\n",
      "Nearest to by: Matteau, watchers, FTPMAIL, Leverrier, barcoded, adept, Spacepac, bugger,\n",
      "Nearest to get: parabolic, Towers, BILLION, coastlands, CSS, repairs, solemn, Roth,\n",
      "Nearest to or: cease, FrequentlyAsked, Into, Kalamazoo, unveiled, withheld, FFL, Runtime,\n",
      "Nearest to more: AllStar, XDMCP, Weight, Whosoever, CDIN, strawman, bindings, ministries,\n",
      "Nearest to up: Kingsley, ALS, peaks, ASPI4DOS, 041893, Feigenbaum, chum, Tax,\n",
      "Nearest to your: Deputy, divisional, mournful, works, ranged, Davis, WarsCivil, vs,\n",
      "Nearest to who: xmosaic, 5k, directors, goin, AutoWeek, Edition, fates, pharmaceutical,\n",
      "Nearest to X: particular, Spencer, deterministic, Automap, sermon, logia, MILITECH, grass,\n",
      "Nearest to can: Drysdale, yourself, imperialist, Rasulovs, guarded, sovereignty, M514514514514514514514514514514514514514514514, cracked,\n",
      "Nearest to was: clipped, Name, IRQ4, divorced, VMax, finished, VMS, accomplishes,\n",
      "Nearest to then: endorsed, Proceedings, Omega, RESPONSIBLE, StephaneRicher, Comeback, spotty, democracy,\n",
      "Nearest to my: launches, Chernomyrdin, Prince, payable, threats, Appressian, Libraries, prefix,\n",
      "Nearest to this: Konroyd, XmNdialogStyle, Soul, PowerGlove, athletes, except, Calgary, 0334,\n",
      "Nearest to UNK: HPP, communal, Find, commander, Levin, concepts, MINISTRY, openbuildbname,\n",
      "Nearest to just: morale, 9192, oneupmanship, Rohrbacher, UVS, characteristically, sufficient, 1140,\n",
      "Average loss at step  2000 :  118.19521633386611\n",
      "Average loss at step  4000 :  55.31689385795593\n",
      "Average loss at step  6000 :  59.17461260485649\n",
      "Average loss at step  8000 :  25.754735862731934\n",
      "Average loss at step  10000 :  22.529167549729348\n",
      "Nearest to The: the, and, for, your, A, Acton, graphicdisplay, supreme,\n",
      "Nearest to by: Plus, from, in, BIT, for, and, width, of,\n",
      "Nearest to get: kt, DUP0, JLE, solemn, karl, graphicdisplay, kg, width,\n",
      "Nearest to or: and, hit, cease, 16256, 0x01, PRINTER, 0000, 0067,\n",
      "Nearest to more: AllStar, Run, DUP0, bindings, dispute, a, woman, Mix,\n",
      "Nearest to up: ASPI4DOS, hit, Are, rrr, federal, this, demonstrated, Kekules,\n",
      "Nearest to your: works, The, the, MOV, Asimov, grounds, width, distribution,\n",
      "Nearest to who: ending, and, directors, or, graphicdisplay, teaching, says, failure,\n",
      "Nearest to X: 0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ, width, BIT, IK, particular, MVI, STA, DJ,\n",
      "Nearest to can: yourself, would, rrr, could, MLB, generator, composed, engine,\n",
      "Nearest to was: is, would, DJ, seems, has, time, sheath, wife,\n",
      "Nearest to then: Omega, JJ, 541969, MVI, v, constant, eatchar, dealer,\n",
      "Nearest to my: Council, 197576, MAS, the, a, launches, apex, IDLE,\n",
      "Nearest to this: the, these, rrr, it, limitations, a, MOV, authcnt,\n",
      "Nearest to UNK: BIT, IK, MVI, JLE, width, STA, DUP0, and,\n",
      "Nearest to just: rrr, would, 9192, mile, MVI, When, them, sufficient,\n",
      "Average loss at step  12000 :  19.390896786928177\n",
      "Average loss at step  14000 :  14.626855673789978\n",
      "Average loss at step  16000 :  12.998343197703361\n",
      "Average loss at step  18000 :  10.06828912615776\n",
      "Average loss at step  20000 :  11.493293567717075\n",
      "Nearest to The: the, and, for, A, supreme, UNK, 1, xxxx,\n",
      "Nearest to by: from, in, for, and, with, UNK, have, is,\n",
      "Nearest to get: have, DUP0, karl, kt, possessed, coastlands, kg, make,\n",
      "Nearest to or: and, UNK, for, XXXXX, Vernor, 0067, as, gw,\n",
      "Nearest to more: UNK, bindings, AllStar, Run, xxxx, Baen, 960, fatally,\n",
      "Nearest to up: firepower, federal, demonstrated, Kekules, hit, rrr, Are, Rescue,\n",
      "Nearest to your: the, their, my, his, a, xxxx, The, works,\n",
      "Nearest to who: and, scuffed, digging, that, 41160, directors, ending, Vernon,\n",
      "Nearest to X: UNK, gb, definekey, BIT, width, IK, XXXXX, xxxx,\n",
      "Nearest to can: will, would, should, could, to, may, cant, yourself,\n",
      "Nearest to was: is, has, and, XXXXX, definekey, by, DJ, but,\n",
      "Nearest to then: XXXXX, Omega, and, UNK, liblibX11so, JJ, 541969, Asian,\n",
      "Nearest to my: the, your, his, nonvalist, Council, xxxx, a, Baen,\n",
      "Nearest to this: it, the, a, these, UNK, any, XXXXX, Zeos,\n",
      "Nearest to UNK: IK, MVI, XXXXX, BIT, definekey, and, gb, width,\n",
      "Nearest to just: rrr, Superman, mile, XrmValuePtr, but, ku, and, MVI,\n",
      "Average loss at step  22000 :  8.007559285759926\n",
      "Average loss at step  24000 :  11.427551832675935\n",
      "Average loss at step  26000 :  8.703297400951385\n",
      "Average loss at step  28000 :  6.6936018387675285\n",
      "Average loss at step  30000 :  36.02435203200579\n",
      "Nearest to The: and, A, the, If, 1, or, of, xxxx,\n",
      "Nearest to by: from, with, in, be, was, on, were, for,\n",
      "Nearest to get: make, be, ech, Conestoga, coastlands, possessed, try, and,\n",
      "Nearest to or: and, a, Hungarian, UNK, XXXXX, for, Vernor, 0067,\n",
      "Nearest to more: less, AllStar, fatally, bindings, CDIN, Run, a, dispute,\n",
      "Nearest to up: federal, firepower, demonstrated, Kingsley, indirect, Rescue, fast, their,\n",
      "Nearest to your: my, their, his, a, our, the, this, these,\n",
      "Nearest to who: Liver, They, digging, morphing, 41160, almost, kc, directors,\n",
      "Nearest to X: gb, particular, PN, definekey, pt, sermon, IK, Brockton,\n",
      "Nearest to can: will, would, could, should, may, to, cant, yourself,\n",
      "Nearest to was: is, has, were, but, and, be, are, And,\n",
      "Nearest to then: Omega, spotty, since, Asian, did, And, Stillwater, Comeback,\n",
      "Nearest to my: your, his, their, our, nonvalist, Council, a, this,\n",
      "Nearest to this: it, a, any, cubs, these, kc, rrr, some,\n",
      "Nearest to UNK: Spherical, PISTOL, MAXAXAXAXAXAXAXAXAXAXAXAXAXAXAX, liblibX11so, MVI, width, definekey, xxxx,\n",
      "Nearest to just: and, but, it, Superman, rrr, a, XrmValuePtr, mile,\n",
      "Average loss at step  32000 :  36.94190920877457\n",
      "Average loss at step  34000 :  14.762374883413315\n",
      "Average loss at step  36000 :  8.422602487564086\n",
      "Average loss at step  38000 :  115.8816198527813\n",
      "Average loss at step  40000 :  13.056234097480774\n",
      "Nearest to The: the, If, for, A, or, In, of, as,\n",
      "Nearest to by: from, for, have, were, with, had, be, as,\n",
      "Nearest to get: make, have, use, see, need, want, be, find,\n",
      "Nearest to or: and, for, The, than, as, of, In, the,\n",
      "Nearest to more: some, there, less, UNK, any, they, you, two,\n",
      "Nearest to up: off, just, us, out, back, this, all, them,\n",
      "Nearest to your: my, their, these, xxxx, address, our, to, toValaddr,\n",
      "Nearest to who: they, They, you, there, these, not, There, that,\n",
      "Nearest to X: 0, Incompleteness, got, XrmValuePtr, package, PN, xxxx, 0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ,\n",
      "Nearest to can: will, could, would, should, may, cant, must, might,\n",
      "Nearest to was: is, has, had, seems, were, would, be, makes,\n",
      "Nearest to then: And, now, so, said, that, But, They, after,\n",
      "Nearest to my: your, his, our, their, a, these, this, any,\n",
      "Nearest to this: it, these, some, that, any, there, no, which,\n",
      "Nearest to UNK: there, you, they, What, They, these, more, not,\n",
      "Nearest to just: really, true, well, all, something, also, now, them,\n",
      "Average loss at step  42000 :  25.485079622149467\n",
      "Average loss at step  44000 :  11.925760723352433\n",
      "Average loss at step  46000 :  9.145090938329696\n",
      "Average loss at step  48000 :  23.85692327734828\n",
      "Average loss at step  50000 :  8.488106147527695\n",
      "Nearest to The: A, the, and, UNK, This, If, but, as,\n",
      "Nearest to by: from, were, in, are, with, be, UNK, is,\n",
      "Nearest to get: make, want, use, do, give, need, see, have,\n",
      "Nearest to or: and, but, for, UNK, If, This, The, as,\n",
      "Nearest to more: less, longer, better, some, any, others, no, Or,\n",
      "Nearest to up: off, out, us, back, it, them, possible, just,\n",
      "Nearest to your: my, their, his, the, our, a, My, her,\n",
      "Nearest to who: They, that, almost, He, I, It, these, ones,\n",
      "Nearest to X: code, system, gb, government, Dune, xxxx, definekey, 1,\n",
      "Nearest to can: will, would, could, should, may, cant, to, must,\n",
      "Nearest to was: is, has, were, seems, had, be, am, Im,\n",
      "Nearest to then: now, And, said, and, you, after, them, so,\n",
      "Nearest to my: your, his, their, our, the, a, this, My,\n",
      "Nearest to this: it, that, any, you, a, every, one, my,\n",
      "Nearest to UNK: and, A, 7, him, The, issue, etc, or,\n",
      "Nearest to just: really, but, true, it, not, still, possible, quite,\n",
      "Average loss at step  52000 :  20.780221069931983\n",
      "Average loss at step  54000 :  7.769398509979248\n",
      "Average loss at step  56000 :  6.879419199228287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  58000 :  10.219114861667157\n",
      "Average loss at step  60000 :  6.9278001153171065\n",
      "Nearest to The: If, This, A, the, but, What, To, But,\n",
      "Nearest to by: from, were, with, be, through, for, are, as,\n",
      "Nearest to get: make, want, have, use, need, give, do, find,\n",
      "Nearest to or: and, but, The, If, for, UNK, Or, about,\n",
      "Nearest to more: less, longer, better, others, some, you, posted, no,\n",
      "Nearest to up: off, UNK, back, them, out, us, just, him,\n",
      "Nearest to your: my, their, his, the, our, her, My, these,\n",
      "Nearest to who: He, They, that, What, We, almost, already, It,\n",
      "Nearest to X: program, UNK, available, The, server, 3, possible, Windows,\n",
      "Nearest to can: will, could, would, should, cant, may, didnt, wont,\n",
      "Nearest to was: is, were, has, had, am, wasnt, seems, are,\n",
      "Nearest to then: And, now, UNK, said, so, them, we, it,\n",
      "Nearest to my: your, his, their, our, the, My, a, these,\n",
      "Nearest to this: it, you, that, I, these, we, so, some,\n",
      "Nearest to UNK: it, now, them, then, once, goes, is, his,\n",
      "Nearest to just: it, also, them, UNK, really, now, but, still,\n",
      "Average loss at step  62000 :  30.45922354698181\n",
      "Average loss at step  64000 :  11.723140625953674\n",
      "Average loss at step  66000 :  49.700130627036096\n",
      "Average loss at step  68000 :  7.0179542444944385\n",
      "Average loss at step  70000 :  49.80296929401159\n",
      "Nearest to The: If, A, This, In, for, 2, For, or,\n",
      "Nearest to by: from, for, through, in, as, with, were, on,\n",
      "Nearest to get: make, use, need, give, do, got, want, see,\n",
      "Nearest to or: for, and, If, Or, The, than, My, Maybe,\n",
      "Nearest to more: less, longer, better, most, others, some, any, 2,\n",
      "Nearest to up: off, back, out, us, down, them, completely, him,\n",
      "Nearest to your: their, my, his, our, her, and, its, to,\n",
      "Nearest to who: They, He, We, which, already, almost, It, else,\n",
      "Nearest to X: J13, UNK, J14, 7120A, Spherical, Goretti, definekey, Hamza,\n",
      "Nearest to can: could, will, should, would, may, cant, must, wont,\n",
      "Nearest to was: is, were, has, had, be, wasnt, am, seems,\n",
      "Nearest to then: said, since, now, before, goes, because, Ill, so,\n",
      "Nearest to my: your, his, their, our, and, her, a, its,\n",
      "Nearest to this: it, any, every, a, you, that, I, what,\n",
      "Nearest to UNK: J14, J13, X, PISTOL, Spherical, 7120A, Hamza, liblibX11so,\n",
      "Nearest to just: also, really, it, them, still, Just, now, get,\n",
      "Average loss at step  72000 :  8.110223730921746\n",
      "Average loss at step  74000 :  22.69231924176216\n",
      "Average loss at step  76000 :  8.821866483926772\n",
      "Average loss at step  78000 :  30.801359000384807\n",
      "Average loss at step  80000 :  174.0691347875595\n",
      "Nearest to The: A, and, If, for, This, in, Code, All,\n",
      "Nearest to by: be, been, was, hardware, from, being, for, as,\n",
      "Nearest to get: use, make, give, want, be, find, need, try,\n",
      "Nearest to or: UNK, MAXAXAXAXAXAXAXAXAXAXAXAXAXAXAX, type, J14, J13, Goretti, PISTOL, Spherical,\n",
      "Nearest to more: less, longer, very, some, better, most, 1, As,\n",
      "Nearest to up: off, out, back, completely, us, down, him, just,\n",
      "Nearest to your: my, his, their, our, its, her, My, is,\n",
      "Nearest to who: They, almost, It, already, What, which, I, He,\n",
      "Nearest to X: Incompleteness, Dune, toValaddr, Kevs, J13, 0, gb, DISCOS,\n",
      "Nearest to can: will, should, could, would, may, might, cant, must,\n",
      "Nearest to was: is, has, seems, had, wasnt, but, isnt, theyre,\n",
      "Nearest to then: said, now, since, and, Ill, after, when, if,\n",
      "Nearest to my: your, his, their, our, its, her, My, a,\n",
      "Nearest to this: it, the, you, is, that, one, a, This,\n",
      "Nearest to UNK: J13, MAXAXAXAXAXAXAXAXAXAXAXAXAXAXAX, J14, type, Spherical, PISTOL, 7120A, Hamza,\n",
      "Nearest to just: really, also, still, them, sufficient, possible, something, she,\n",
      "Average loss at step  82000 :  19.798600990056993\n",
      "Average loss at step  84000 :  35.84704492866993\n",
      "Average loss at step  86000 :  11.991489654779434\n",
      "Average loss at step  88000 :  10.214289600372314\n",
      "Average loss at step  90000 :  94.88615132188797\n",
      "Nearest to The: of, This, and, If, For, in, from, get,\n",
      "Nearest to by: through, when, been, being, were, be, had, are,\n",
      "Nearest to get: ra, him, loveth, from, DMA, Motif, HE, suffer,\n",
      "Nearest to or: and, UNK, have, do, may, much, ask, should,\n",
      "Nearest to more: less, better, longer, some, most, UNK, would, very,\n",
      "Nearest to up: off, back, do, just, us, out, down, only,\n",
      "Nearest to your: him, DMA, ones, such, available, already, bike, tell,\n",
      "Nearest to who: They, they, He, else, actually, We, still, You,\n",
      "Nearest to X: UNK, program, discussion, right, particular, only, them, do,\n",
      "Nearest to can: get, ra, him, Motif, from, loveth, should, DMA,\n",
      "Nearest to was: had, be, is, has, have, just, wasnt, should,\n",
      "Nearest to then: now, if, when, said, since, we, they, after,\n",
      "Nearest to my: his, their, our, your, its, My, her, UNK,\n",
      "Nearest to this: it, what, some, one, UNK, which, all, only,\n",
      "Nearest to UNK: do, have, them, us, should, people, or, may,\n",
      "Nearest to just: them, have, do, may, something, should, why, UNK,\n",
      "Average loss at step  92000 :  18.171992664515972\n",
      "Average loss at step  94000 :  49.0869389783144\n",
      "Average loss at step  96000 :  146.6346380414963\n",
      "Average loss at step  98000 :  129.01758806288242\n",
      "Average loss at step  100000 :  40.475448055744174\n",
      "Nearest to The: the, This, A, In, It, And, but, If,\n",
      "Nearest to by: be, after, to, name, sometimes, without, among, rate,\n",
      "Nearest to get: make, want, give, need, go, do, take, have,\n",
      "Nearest to or: and, but, only, does, do, sure, Maybe, did,\n",
      "Nearest to more: some, less, to, rather, t, longer, WW, among,\n",
      "Nearest to up: off, back, down, out, into, together, them, him,\n",
      "Nearest to your: my, their, our, his, her, My, any, its,\n",
      "Nearest to who: He, They, still, It, they, There, almost, I,\n",
      "Nearest to X: application, exist, 31, 3, discussion, typical, internet, Code,\n",
      "Nearest to can: will, could, should, wouldnt, wont, must, would, to,\n",
      "Nearest to was: is, has, seems, did, had, wasnt, were, but,\n",
      "Nearest to then: Ill, now, you, also, so, did, if, since,\n",
      "Nearest to my: your, his, our, their, My, her, its, a,\n",
      "Nearest to this: one, you, another, a, here, This, every, some,\n",
      "Nearest to UNK: what, But, post, that, Who, though, question, Now,\n",
      "Nearest to just: also, really, shall, them, think, still, probably, only,\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Begin training.\n",
    "num_steps = 100001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  # Open a writer to write summaries.\n",
    "  writer = tf.summary.FileWriter(FLAGS.log_dir, session.graph)\n",
    "\n",
    "  # We must initialize all variables before we use them.\n",
    "  init.run()\n",
    "  print('Initialized')\n",
    "\n",
    "  average_loss = 0\n",
    "  for step in xrange(num_steps):\n",
    "    batch_inputs, batch_labels = generate_batch(batch_size, num_skips,\n",
    "                                                skip_window)\n",
    "    feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "    # Define metadata variable.\n",
    "    run_metadata = tf.RunMetadata()\n",
    "\n",
    "    # We perform one update step by evaluating the optimizer op (including it\n",
    "    # in the list of returned values for session.run()\n",
    "    # Also, evaluate the merged op to get all summaries from the returned \"summary\" variable.\n",
    "    # Feed metadata variable to session for visualizing the graph in TensorBoard.\n",
    "    _, summary, loss_val = session.run(\n",
    "        [optimizer, merged, loss],\n",
    "        feed_dict=feed_dict,\n",
    "        run_metadata=run_metadata)\n",
    "    average_loss += loss_val\n",
    "\n",
    "    # Add returned summaries to writer in each step.\n",
    "    writer.add_summary(summary, step)\n",
    "    # Add metadata to visualize the graph for the last run.\n",
    "    if step == (num_steps - 1):\n",
    "      writer.add_run_metadata(run_metadata, 'step%d' % step)\n",
    "\n",
    "    if step % 2000 == 0:\n",
    "      if step > 0:\n",
    "        average_loss /= 2000\n",
    "      # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "      print('Average loss at step ', step, ': ', average_loss)\n",
    "      average_loss = 0\n",
    "\n",
    "    # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "    if step % 10000 == 0:\n",
    "      sim = similarity.eval()\n",
    "      for i in xrange(valid_size):\n",
    "        valid_word = reverse_dictionary[valid_examples[i]]\n",
    "        top_k = 8  # number of nearest neighbors\n",
    "        nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "        log_str = 'Nearest to %s:' % valid_word\n",
    "        for k in xrange(top_k):\n",
    "          close_word = reverse_dictionary[nearest[k]]\n",
    "          log_str = '%s %s,' % (log_str, close_word)\n",
    "        print(log_str)\n",
    "  final_embeddings = normalized_embeddings.eval()\n",
    "\n",
    "  # Write corresponding labels for the embeddings.\n",
    "  with open(FLAGS.log_dir + '/metadata.tsv', 'w') as f:\n",
    "    for i in xrange(vocabulary_size):\n",
    "      f.write(reverse_dictionary[i] + '\\n')\n",
    "\n",
    "  # Save the model for checkpoints.\n",
    "  saver.save(session, os.path.join(FLAGS.log_dir, 'model.ckpt'))\n",
    "\n",
    "  # Create a configuration for visualizing embeddings with the labels in TensorBoard.\n",
    "  config = projector.ProjectorConfig()\n",
    "  embedding_conf = config.embeddings.add()\n",
    "  embedding_conf.tensor_name = embeddings.name\n",
    "  embedding_conf.metadata_path = os.path.join(FLAGS.log_dir, 'metadata.tsv')\n",
    "  projector.visualize_embeddings(writer, config)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Visualize the embeddings.\n",
    "\n",
    "\n",
    "# pylint: disable=missing-docstring\n",
    "# Function to draw visualization of distance between embeddings.\n",
    "def plot_with_labels(low_dim_embs, labels, filename):\n",
    "  assert low_dim_embs.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "  plt.figure(figsize=(18, 18))  # in inches\n",
    "  for i, label in enumerate(labels):\n",
    "    x, y = low_dim_embs[i, :]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(\n",
    "        label,\n",
    "        xy=(x, y),\n",
    "        xytext=(5, 2),\n",
    "        textcoords='offset points',\n",
    "        ha='right',\n",
    "        va='bottom')\n",
    "\n",
    "  plt.savefig(filename)\n",
    "\n",
    "\n",
    "try:\n",
    "  # pylint: disable=g-import-not-at-top\n",
    "  from sklearn.manifold import TSNE\n",
    "  import matplotlib.pyplot as plt\n",
    "\n",
    "  tsne = TSNE(\n",
    "      perplexity=30, n_components=2, init='pca', n_iter=5000, method='exact')\n",
    "  plot_only = 500\n",
    "  low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])\n",
    "  labels = [reverse_dictionary[i] for i in xrange(plot_only)]\n",
    "  plot_with_labels(low_dim_embs, labels, os.path.join(gettempdir(), 'tsne.png'))\n",
    "\n",
    "except ImportError as ex:\n",
    "  print('Please install sklearn, matplotlib, and scipy to show embeddings.')\n",
    "  print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/var/folders/d0/vtcq_yxd4_l06mjb3n9wmb0m0000gn/T'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gettempdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In\n"
     ]
    }
   ],
   "source": [
    "print(reverse_dictionary[90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "sim1 = cosine_similarity(final_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 1\n",
      "Nearest to the: The, going, youre, these, even, and, did, people, common, you, only, probably, likely, obvious, think, but, its, yet, certainly, every,\n"
     ]
    }
   ],
   "source": [
    "valid_word = 'the'\n",
    "key = 0\n",
    "for k,v in reverse_dictionary.items():\n",
    "    if v == valid_word:\n",
    "        key = k\n",
    "        break\n",
    "i=key\n",
    "top_k = 20  # number of nearest neighbors\n",
    "print(\"i:\",i)\n",
    "\n",
    "nearest = (-sim1[i, :]).argsort()[1:top_k + 1]\n",
    "log_str = 'Nearest to %s:' % valid_word\n",
    "for k in xrange(top_k):\n",
    "    close_word = reverse_dictionary[nearest[k]]\n",
    "    log_str = '%s %s,' % (log_str, close_word)\n",
    "print(log_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
